# -*- coding: utf-8 -*-
"""SVM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BgnpacaTuQ-gYyHCywA99f5cFGdUOAG4

## SVM 101

Support vector machine is used as a supervised learning algorithm in ML for classification and regression use.

SVM can be quite useful when it comes to binary problems such as classify elements between two classes.

Please use this code as you please #open-source
"""

# @title Basic SVM code #open-source
# @markdown Author: Marc Haraoui

# @markdown Year of update: 2023

"""## Create the data

### Import main modules
"""

import numpy as np
import matplotlib.pyplot as plt
import cvxopt
from sklearn import svm
from random import gauss

"""### Data creation"""

# Plot function
def plot_data(x, y, xlength, ylength,radius):
    plt.scatter(x[:,0], x[:,1], c=y, s=radius, cmap='viridis')
    plt.xlim(xlength)
    plt.ylim(ylength)

x = np.array([[0, 3], [3, 1], [0, 0],[0, 1]])

y = np.array([-1, -1, 1, 1]) # first 2 points are from class C1, and the other 2 are in C2 (y is used for coloring the points)

# Print data info
print(f"All points coordination: \n{x}")
print(f"Points coordination in x axis: \n{x[:,0]}")
print(f"Points coordination in x axis: \n{x[:,1]}")

# Plotting the data
plot_data(x, y, (-1,4), (-1,4), 100)

"""## Linear SVM in primal form

### All functions used
"""

def plot(x,y):
  plot_data(x,y, (-1,4), (-1,4), 100)

def linear_seperator(w, b, xlength, ylength):
    x1 = xlength[0]
    y1 = -(b+(w[0]*x1))/w[1]
    x2 = xlength[1]
    y2 = -(b+(w[0]*x2))/w[1]

    plt.plot((x1, x2), (y1, y2))
    plt.xlim(xlength)
    plt.ylim(ylength)

def primal(x,y, C=1):
    N = x.shape[0]
    n = x.shape[1]

    q  = np.concatenate((np.zeros((n+1,1)),C*np.ones((N,1))),axis = 0)
    q  = cvxopt.matrix(q)
    P1 = np.zeros((1, 1+n+N))
    P2 = np.concatenate((np.zeros((n,1)),np.eye(n), np.zeros((n,N))),axis=1)
    P3 = np.zeros((N, 1+n+N))
    P  = np.concatenate((P1, P2, P3),axis=0)
    P  = cvxopt.matrix(P)

    for i in range(N):
        g = np.concatenate((np.reshape(-y[i],(1,1)), np.reshape(-y[i]*x[i][:],(1,2))),axis=1)
        if i==0:
            G = g
        else:
            G = np.concatenate((G, g), axis=0)

    G  = np.concatenate((G,-np.eye(N)),axis=1)
    G2 = np.concatenate((np.zeros((N,n+1)),-np.eye(N)),axis = 1)
    G  = np.concatenate((G,G2),axis =0)
    G  = cvxopt.matrix(G+0.)

    h   = np.concatenate((-np.ones((N,1)),np.zeros((N,1))),axis =0)
    h   = cvxopt.matrix(h)
    ret = cvxopt.solvers.qp(P, q, G, h)
    w   = [ret['x'][1],ret['x'][2]]
    b   = ret['x'][0]

    return w, b

"""### Linear seperator in primal"""

# Play around with x to see how the points affect the seperator
x = np.array([[1, 3], [3, 1], [0, 0],[0, 1]])
y = np.array([-1,-1,1,1])

w, b = primal(x,y)
print(w , b)

fig = plt.figure()
linear_seperator(w,b,(-1,4),(-1,4))
plot(x,y)

# Play around with x to see how the points affect the seperator
x = np.array([[1, 3], [3, 1], [0, 0],[0, 1]])
y = np.array([-1,-1,1,1])

# Add more points to explore more
X = np.concatenate((x,[[2, 2]]), axis=0)
Y = np.concatenate((y,[-1]), axis=0) # choose which class your point is for

w, b = primal(X,Y)
print(w , b)

fig = plt.figure()
linear_seperator(w,b,(-1,4),(-1,4))
plot(X,Y)

"""You have probably guessed it by now, this algorithm is impressive but limited to simple cases.

There are cases where a linear seperator cannot be obtained.

Feel free to run the following code to see an example of this
"""

Y = np.concatenate((y,[1]), axis=0) # choose which class your point is for

w, b = primal(X,Y)
print(w , b)

fig = plt.figure()
linear_seperator(w,b,(-1,4),(-1,4))
plot(X,Y)

"""### Using Kernel SVM

Using Kernel SVM allows us to deal with non linear and higher dimensions. For this example we will be using hyperplanes using **svm** from sklearn.

## All functions used
"""

def kernel_seperator(x, y, model, xlength=(-1,4), ylength=(-1,4)):
  plot_data(x,y, xlength, ylength, 100)
  xx, yy = np.meshgrid(np.linspace(xlength[0], xlength[1],50), np.linspace(ylength[0], ylength[1],100))
  xy = np.concatenate((np.reshape(xx,(xx.shape[0]*xx.shape[1],1)),np.reshape(yy,(yy.shape[0]*yy.shape[1],1))),axis=1)
  P = model.predict(xy)
  plot_data(xy,P,xlength,ylength,1)

"""Kernel seperator"""

# Play around with x to see how the points affect the seperator
x = np.array([[1, 3], [3, 1], [0, 0],[0, 1]])
y = np.array([-1,-1,1,1])

# Add more points to explore more
X = np.concatenate((x,[[2, 2]]), axis=0)
Y = np.concatenate((y,[-1]), axis=0) # choose which class your point is for

model = svm.SVC(kernel='rbf', C=10)
model.fit(X, Y)
fig = plt.figure()
kernel_seperator(X, Y, model)

"""Notice how a linear SVM would have done a good job seperating the two classes here. It is important to know which type of SVM to use depending on the case."""

Y = np.concatenate((y,[1]), axis=0) # choose which class your point is for

model = svm.SVC(kernel='rbf', C=10)
model.fit(X, Y)
fig = plt.figure()
kernel_seperator(X, Y, model)

"""Here we can clearly see a good use of a Kernel SVM. Our points are well seperated based on their respectful classes."""